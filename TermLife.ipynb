{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "term_life = pd.read_csv('TermLife.csv')\n",
    "\n",
    "# These are the relevant columns we are checking\n",
    "insurance_columns = ['FACE', 'FACECVLIFEPOLICIES', 'CASHCVLIFEPOLICIES', 'BORROWCVLIFEPOL', 'NETVALUE']\n",
    "\n",
    "# Create a new column 'PURCHASED' where 1 indicates life insurance was purchased and 0 otherwise\n",
    "term_life['PURCHASED'] = (term_life[insurance_columns].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# Check the data types and the first few rows to understand the structure\n",
    "print(term_life.info())\n",
    "print(term_life.head())\n",
    "\n",
    "# Define the variable types based on your description\n",
    "binary_vars = ['GENDER', 'SGENDER', 'BORROWCVLIFEPOL', 'NETVALUE']  # Binary variables\n",
    "nominal_vars = ['MARSTAT', 'ETHNICITY', 'SMARSTAT']  # Nominal variables\n",
    "target_vars = ['PURCHASED']\n",
    "continuous_vars = [col for col in term_life.columns if col not in binary_vars + nominal_vars + target_vars]  # All others are continuous\n",
    "term_life_encoded = pd.get_dummies(term_life, columns=nominal_vars, drop_first=True)\n",
    "\n",
    "# Convert binary variables to 'category' type\n",
    "for var in binary_vars:\n",
    "    term_life[var] = term_life[var].astype('category')\n",
    "\n",
    "# Convert nominal variables to 'category' type\n",
    "for var in nominal_vars:\n",
    "    term_life[var] = term_life[var].astype('category')\n",
    "    \n",
    "# Separate features and target variable\n",
    "X = term_life_encoded.drop('PURCHASED', axis=1)  # Features\n",
    "y = term_life_encoded['PURCHASED']  # Target variable (binary)\n",
    "\n",
    "# Split the data into train and test sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the continuous variables only\n",
    "scaler = StandardScaler()\n",
    "X_train_cont_scaled = scaler.fit_transform(X_train[continuous_vars])\n",
    "X_test_cont_scaled = scaler.transform(X_test[continuous_vars])\n",
    "\n",
    "# Combine scaled continuous variables with unscaled binary and nominal variables\n",
    "X_train_scaled = pd.DataFrame(X_train_cont_scaled, columns=continuous_vars, index=X_train.index)\n",
    "X_train_scaled = pd.concat([X_train_scaled, X_train.drop(columns=continuous_vars)], axis=1)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_cont_scaled, columns=continuous_vars, index=X_test.index)\n",
    "X_test_scaled = pd.concat([X_test_scaled, X_test.drop(columns=continuous_vars)], axis=1)\n",
    "\n",
    "# Print results\n",
    "print(term_life_encoded.head())\n",
    "print(X_train_scaled.head())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Ensure all columns in X_train_with_constant are numeric\n",
    "# If any column is non-numeric, you can check its type and handle it accordingly\n",
    "X_train_with_constant = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "# Check if all columns are numeric\n",
    "non_numeric_cols = X_train_with_constant.select_dtypes(exclude=['number']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# If there are non-numeric columns, convert them or drop them (you may convert them to dummy variables if appropriate)\n",
    "if len(non_numeric_cols) > 0:\n",
    "    X_train_with_constant = X_train_with_constant.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Add a constant for the intercept (if not already added)\n",
    "X_train_with_constant['Intercept'] = 1\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_train_with_constant.columns\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_with_constant.values, i) for i in range(X_train_with_constant.shape[1])]\n",
    "\n",
    "# Display the VIF values\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# Convert numpy arrays back to DataFrames, using the original feature names from X_train\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "# Ensure that VIF is calculated only on the feature columns (no intercept)\n",
    "vif_data = vif_data[vif_data['Feature'] != 'Intercept']\n",
    "\n",
    "# Identify variables with VIF > 5 or infinite VIF\n",
    "high_vif_vars = vif_data[vif_data['VIF'] > 5]['Feature'].tolist()\n",
    "\n",
    "# Now, drop the high VIF variables from both training and test sets\n",
    "X_train_reduced = X_train_scaled_df.drop(columns=high_vif_vars)\n",
    "X_test_reduced = X_test_scaled_df.drop(columns=high_vif_vars)\n",
    "\n",
    "# Logistic regression without regularization\n",
    "logistic_model = LogisticRegression(penalty= None, max_iter=1000, solver='lbfgs').fit(X_train_reduced, y_train)\n",
    "\n",
    "# Print coefficients with corresponding feature names\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,  # Feature names\n",
    "    'Coefficient': logistic_model.coef_[0]  # Coefficients (assuming binary classification)\n",
    "})\n",
    "\n",
    "print(coefficients)\n",
    "\n",
    "# Predictions\n",
    "y_pred_logistic = logistic_model.predict(X_test_reduced)\n",
    "\n",
    "\n",
    "# Calculate log-loss (deviance)\n",
    "log_loss_train = log_loss(y_train, logistic_model.predict_proba(X_train_reduced))\n",
    "log_loss_test = log_loss(y_test, logistic_model.predict_proba(X_test_reduced))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "print(\"Training Log Loss (Deviance):\", 2 * log_loss_train)\n",
    "print(\"Test Log Loss (Deviance):\", 2 * log_loss_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compute null deviance\n",
    "def null_deviance(y):\n",
    "    N = len(y)\n",
    "    p = y.mean()\n",
    "    null_log_likelihood = N * (np.log(p) * p + np.log(1 - p) * (1 - p))\n",
    "    return -2 * null_log_likelihood  # Null Deviance\n",
    "\n",
    "# Deviance (already computed from log_loss)\n",
    "deviance_train = 2 * log_loss_train\n",
    "deviance_test = 2 * log_loss_test\n",
    "\n",
    "# Null deviance for training and test sets\n",
    "null_deviance_train = null_deviance(y_train)\n",
    "null_deviance_test = null_deviance(y_test)\n",
    "\n",
    "# Compute R^2 for training and test sets\n",
    "r2_train = 1 - (deviance_train / null_deviance_train)\n",
    "r2_test = 1 - (deviance_test / null_deviance_test)\n",
    "\n",
    "print(\"Training R²:\", r2_train)\n",
    "print(\"Test R²:\", r2_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Lasso Logistic Regression (L1)\n",
    "lasso_model = LogisticRegressionCV(cv=10, penalty='l1', solver='saga', max_iter=10000).fit(X_train_reduced, y_train)\n",
    "coefficient_lasso = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,  # Feature names\n",
    "    'Coefficient': lasso_model.coef_[0]  # Coefficients (assuming binary classification)\n",
    "})\n",
    "print(\"Ridge Coefficients:\", coefficient_lasso)\n",
    "print(\"Best C for LASSO:\", lasso_model.C_)\n",
    "\n",
    "# Ridge Logistic Regression (L2)\n",
    "ridge_model = LogisticRegressionCV(cv=10, penalty='l2', solver='saga', max_iter=10000).fit(X_train_reduced, y_train)\n",
    "coefficient_ridge = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,  # Feature names\n",
    "    'Coefficient': ridge_model.coef_[0]  # Coefficients (assuming binary classification)\n",
    "})\n",
    "print(\"Ridge Coefficients:\", coefficient_ridge)\n",
    "print(\"Best C for Ridge:\", ridge_model.C_)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, log_loss, r2_score\n",
    "\n",
    "# Predict probabilities for test data using LASSO and Ridge models\n",
    "y_test_pred_probs_lasso = lasso_model.predict_proba(X_test_reduced)[:, 1]\n",
    "y_test_pred_probs_ridge = ridge_model.predict_proba(X_test_reduced)[:, 1]\n",
    "\n",
    "# Calculate deviance (negative log-likelihood * -2) for LASSO and Ridge\n",
    "deviance_lasso = 2 * log_loss(y_test, y_test_pred_probs_lasso)\n",
    "deviance_ridge = 2 * log_loss(y_test, y_test_pred_probs_ridge)\n",
    "\n",
    "# Predict class labels for accuracy and precision calculation\n",
    "y_test_pred_lasso = lasso_model.predict(X_test_reduced)\n",
    "y_test_pred_ridge = ridge_model.predict(X_test_reduced)\n",
    "\n",
    "# Calculate accuracy and precision for LASSO and Ridge\n",
    "accuracy_lasso = accuracy_score(y_test, y_test_pred_lasso)\n",
    "accuracy_ridge = accuracy_score(y_test, y_test_pred_ridge)\n",
    "precision_lasso = precision_score(y_test, y_test_pred_lasso)\n",
    "precision_ridge = precision_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "# Out-of-sample R^2 for LASSO and Ridge\n",
    "r2_lasso = r2_score(y_test, y_test_pred_probs_lasso)\n",
    "r2_ridge = r2_score(y_test, y_test_pred_probs_ridge)\n",
    "\n",
    "performance_metrics = {\n",
    "    'Deviance LASSO': deviance_lasso,\n",
    "    'Deviance Ridge': deviance_ridge,\n",
    "    'Accuracy LASSO': accuracy_lasso,\n",
    "    'Accuracy Ridge': accuracy_ridge,\n",
    "    'Precision LASSO': precision_lasso,\n",
    "    'Precision Ridge': precision_ridge,\n",
    "    'Out-of-sample R^2 LASSO': r2_lasso,\n",
    "    'Out-of-sample R^2 Ridge': r2_ridge\n",
    "}\n",
    "\n",
    "print(performance_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for scikit-learn models (Logistic Regression, Lasso, Ridge)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curve(estimator, X, y, title, cv = 10, scoring = 'neg_log_loss'):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "    )\n",
    "    \n",
    "    train_mean = -train_scores.mean(axis=1)\n",
    "    train_std = train_scores.std(axis=1)\n",
    "    test_mean = -test_scores.mean(axis=1)\n",
    "    test_std = test_scores.std(axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"r\", label=\"Training Loss\")\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color=\"g\", label=\"Validation Loss\")\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"r\", alpha=0.1)\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"g\", alpha=0.1)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Size\")\n",
    "    plt.ylabel(\"Negative Log Loss\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# Logistic regression without regularization\n",
    "plot_learning_curve(logistic_model, X_train_reduced, y_train, title=\"Logistic Regression Learning Curve\")\n",
    "\n",
    "# LASSO Logistic Regression\n",
    "plot_learning_curve(lasso_model, X_train_reduced, y_train, title=\"LASSO Learning Curve\")\n",
    "\n",
    "# Ridge Logistic Regression\n",
    "plot_learning_curve(ridge_model, X_train_reduced, y_train, title=\"Ridge Learning Curve\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
